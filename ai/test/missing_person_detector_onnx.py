"""
ONNX ê¸°ë°˜ ìµœì í™”ëœ ì‹¤ì¢…ì íƒì§€ ì‹œìŠ¤í…œ
- YOLOv8 ONNX: 2-3ë°° ì†ë„ í–¥ìƒ
- OSNet ONNX: 1.5-2ë°° ì†ë„ í–¥ìƒ
- í”„ë ˆì„ ìŠ¤í‚µ: ì„ íƒì  í”„ë ˆì„ ì²˜ë¦¬
- í•´ìƒë„ ë‹¤ìš´ìŠ¤ì¼€ì¼: ë©”ëª¨ë¦¬ ë° ì†ë„ ìµœì í™”
- ë©€í‹°ìŠ¤ë ˆë”©: ë³‘ë ¬ ì²˜ë¦¬
"""

import cv2
import numpy as np
import onnxruntime as ort
from PIL import Image
import time
from concurrent.futures import ThreadPoolExecutor
import torch


class MissingPersonDetectorONNX:
    def __init__(
        self,
        yolo_onnx_path='yolov8n.onnx',
        osnet_onnx_path='osnet_x1_0.onnx',
        similarity_threshold=0.75,
        matching_strategy='average',
        frame_skip=0,  # 0: ëª¨ë“  í”„ë ˆì„, 1: 1í”„ë ˆì„ ê±´ë„ˆë›°ê¸°, 2: 2í”„ë ˆì„ ê±´ë„ˆë›°ê¸°
        resize_factor=1.0,  # 1.0: ì›ë³¸, 0.5: 50% ì¶•ì†Œ
        use_gpu=True
    ):
        """
        ONNX ê¸°ë°˜ ì‹¤ì¢…ì íƒì§€ ì‹œìŠ¤í…œ ì´ˆê¸°í™”

        Args:
            yolo_onnx_path: YOLO ONNX ëª¨ë¸ ê²½ë¡œ
            osnet_onnx_path: OSNet ONNX ëª¨ë¸ ê²½ë¡œ
            similarity_threshold: ìœ ì‚¬ë„ ì„ê³„ê°’
            matching_strategy: ë§¤ì¹­ ì „ëµ ('max', 'average', 'weighted', 'strict')
            frame_skip: í”„ë ˆì„ ìŠ¤í‚µ ê°„ê²© (0=ëª¨ë“  í”„ë ˆì„ ì²˜ë¦¬)
            resize_factor: í•´ìƒë„ ì¶•ì†Œ ë¹„ìœ¨ (0.5 = 50% í¬ê¸°)
            use_gpu: GPU ì‚¬ìš© ì—¬ë¶€
        """
        print("ğŸš€ ONNX ê¸°ë°˜ ìµœì í™” ëª¨ë¸ ë¡œë”© ì¤‘...")

        # ONNX Runtime ì„¤ì •
        self.providers = self._get_providers(use_gpu)
        print(f"   ì‚¬ìš© Provider: {self.providers[0]}")

        # YOLOv8 ONNX ì„¸ì…˜ ìƒì„±
        print(f"   YOLOv8 ONNX ë¡œë”©: {yolo_onnx_path}")
        self.yolo_session = ort.InferenceSession(
            yolo_onnx_path,
            providers=self.providers
        )
        self.yolo_input_name = self.yolo_session.get_inputs()[0].name
        self.yolo_input_shape = self.yolo_session.get_inputs()[0].shape
        print(f"   âœ“ YOLOv8 ì…ë ¥ í¬ê¸°: {self.yolo_input_shape}")

        # OSNet ONNX ì„¸ì…˜ ìƒì„±
        print(f"   OSNet ONNX ë¡œë”©: {osnet_onnx_path}")
        self.osnet_session = ort.InferenceSession(
            osnet_onnx_path,
            providers=self.providers
        )
        print(f"   âœ“ OSNet ë¡œë”© ì™„ë£Œ")

        # ì„¤ì •
        self.similarity_threshold = similarity_threshold
        self.matching_strategy = matching_strategy
        self.frame_skip = frame_skip
        self.resize_factor = resize_factor

        # ì‹¤ì¢…ì ì„ë² ë”©
        self.missing_person_embeddings = []

        # ìŠ¤ë ˆë“œ í’€ (ë³‘ë ¬ ì²˜ë¦¬ìš©)
        self.executor = ThreadPoolExecutor(max_workers=2)

        print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!\n")

    def _get_providers(self, use_gpu):
        """ì‚¬ìš© ê°€ëŠ¥í•œ Execution Provider ê²°ì •"""
        if use_gpu and 'CUDAExecutionProvider' in ort.get_available_providers():
            return ['CUDAExecutionProvider', 'CPUExecutionProvider']
        else:
            return ['CPUExecutionProvider']

    def preprocess_yolo(self, image):
        """YOLO ì…ë ¥ ì „ì²˜ë¦¬"""
        # ì›ë³¸ í¬ê¸° ì €ì¥
        orig_h, orig_w = image.shape[:2]

        # í•´ìƒë„ ì¶•ì†Œ (ì˜µì…˜)
        if self.resize_factor != 1.0:
            new_w = int(orig_w * self.resize_factor)
            new_h = int(orig_h * self.resize_factor)
            image = cv2.resize(image, (new_w, new_h))

        # YOLO ì…ë ¥ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
        input_size = self.yolo_input_shape[2]  # 640
        img_resized = cv2.resize(image, (input_size, input_size))

        # BGR to RGB
        img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)

        # ì •ê·œí™” ë° ì°¨ì› ë³€ê²½
        img_normalized = img_rgb.astype(np.float32) / 255.0
        img_transposed = np.transpose(img_normalized, (2, 0, 1))  # HWC -> CHW
        img_batched = np.expand_dims(img_transposed, axis=0)  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€

        return img_batched, (orig_w, orig_h)

    def postprocess_yolo(self, outputs, orig_size, conf_threshold=0.5):
        """YOLO ì¶œë ¥ í›„ì²˜ë¦¬"""
        predictions = outputs[0]  # (1, 84, 8400)
        predictions = predictions[0]  # (84, 8400)

        # ì „ì¹˜ (8400, 84)
        predictions = predictions.T

        # ë°•ìŠ¤ ì¢Œí‘œì™€ í´ë˜ìŠ¤ ì ìˆ˜ ë¶„ë¦¬
        boxes = predictions[:, :4]  # (8400, 4)
        scores = predictions[:, 4:]  # (8400, 80)

        # í´ë˜ìŠ¤ 0 (person)ë§Œ í•„í„°ë§
        person_scores = scores[:, 0]
        mask = person_scores > conf_threshold

        if not np.any(mask):
            return []

        # í•„í„°ë§ëœ ë°•ìŠ¤ì™€ ì ìˆ˜
        boxes = boxes[mask]
        person_scores = person_scores[mask]

        # xywh -> xyxy ë³€í™˜
        boxes_xyxy = self._xywh2xyxy(boxes)

        # ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°ë¡œ ìŠ¤ì¼€ì¼ ì¡°ì •
        orig_w, orig_h = orig_size
        input_size = self.yolo_input_shape[2]

        scale_x = orig_w / input_size
        scale_y = orig_h / input_size

        boxes_xyxy[:, [0, 2]] *= scale_x
        boxes_xyxy[:, [1, 3]] *= scale_y

        # NMS (Non-Maximum Suppression)
        indices = self._nms(boxes_xyxy, person_scores, iou_threshold=0.45)

        results = []
        for idx in indices:
            x1, y1, x2, y2 = boxes_xyxy[idx].astype(int)
            conf = float(person_scores[idx])
            results.append({
                'bbox': [x1, y1, x2, y2],
                'confidence': conf
            })

        return results

    def _xywh2xyxy(self, boxes):
        """ì¤‘ì‹¬ì  í˜•ì‹(xywh)ì„ ì¢Œìƒë‹¨-ìš°í•˜ë‹¨ í˜•ì‹(xyxy)ìœ¼ë¡œ ë³€í™˜"""
        boxes_xyxy = np.copy(boxes)
        boxes_xyxy[:, 0] = boxes[:, 0] - boxes[:, 2] / 2  # x1
        boxes_xyxy[:, 1] = boxes[:, 1] - boxes[:, 3] / 2  # y1
        boxes_xyxy[:, 2] = boxes[:, 0] + boxes[:, 2] / 2  # x2
        boxes_xyxy[:, 3] = boxes[:, 1] + boxes[:, 3] / 2  # y2
        return boxes_xyxy

    def _nms(self, boxes, scores, iou_threshold=0.45):
        """Non-Maximum Suppression"""
        x1 = boxes[:, 0]
        y1 = boxes[:, 1]
        x2 = boxes[:, 2]
        y2 = boxes[:, 3]

        areas = (x2 - x1) * (y2 - y1)
        order = scores.argsort()[::-1]

        keep = []
        while order.size > 0:
            i = order[0]
            keep.append(i)

            xx1 = np.maximum(x1[i], x1[order[1:]])
            yy1 = np.maximum(y1[i], y1[order[1:]])
            xx2 = np.minimum(x2[i], x2[order[1:]])
            yy2 = np.minimum(y2[i], y2[order[1:]])

            w = np.maximum(0.0, xx2 - xx1)
            h = np.maximum(0.0, yy2 - yy1)
            inter = w * h

            iou = inter / (areas[i] + areas[order[1:]] - inter)

            inds = np.where(iou <= iou_threshold)[0]
            order = order[inds + 1]

        return keep

    def extract_embedding(self, image):
        """OSNet ONNXë¡œ ì„ë² ë”© ì¶”ì¶œ"""
        if isinstance(image, np.ndarray):
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            image = Image.fromarray(image)

        # ì „ì²˜ë¦¬
        img_resized = image.resize((128, 256))  # OSNet ì…ë ¥ í¬ê¸°
        img_array = np.array(img_resized).astype(np.float32) / 255.0

        # ì •ê·œí™”
        mean = np.array([0.485, 0.456, 0.406]).reshape(1, 1, 3)
        std = np.array([0.229, 0.224, 0.225]).reshape(1, 1, 3)
        img_normalized = (img_array - mean) / std

        # ì°¨ì› ë³€ê²½ (HWC -> CHW)
        img_transposed = np.transpose(img_normalized, (2, 0, 1))
        img_batched = np.expand_dims(img_transposed, axis=0).astype(np.float32)

        # ì¶”ë¡ 
        outputs = self.osnet_session.run(None, {'input': img_batched})
        features = outputs[0]  # (1, 512)

        # L2 ì •ê·œí™”
        features = features / np.linalg.norm(features, axis=1, keepdims=True)

        return features

    def set_missing_person(self, image):
        """ë‹¨ì¼ ì´ë¯¸ì§€ ì„¤ì •"""
        self.missing_person_embeddings = [self.extract_embedding(image)]

    def set_missing_persons(self, images):
        """ì—¬ëŸ¬ ì´ë¯¸ì§€ ì„¤ì •"""
        self.missing_person_embeddings = []
        for image in images:
            embedding = self.extract_embedding(image)
            self.missing_person_embeddings.append(embedding)

    def compute_similarity(self, embedding):
        """ì‹¤ì¢…ìì™€ì˜ ìœ ì‚¬ë„ ê³„ì‚°"""
        if not self.missing_person_embeddings:
            raise ValueError("ì‹¤ì¢…ì ì´ë¯¸ì§€ë¥¼ ë¨¼ì € ì„¤ì •í•´ì£¼ì„¸ìš”!")

        similarities = []
        for ref_embedding in self.missing_person_embeddings:
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
            similarity = np.dot(embedding, ref_embedding.T)[0][0]
            similarities.append(similarity)

        # ë§¤ì¹­ ì „ëµì— ë”°ë¼ ìµœì¢… ìœ ì‚¬ë„ ê³„ì‚°
        if self.matching_strategy == 'max':
            return max(similarities)
        elif self.matching_strategy == 'average':
            return sum(similarities) / len(similarities)
        elif self.matching_strategy == 'weighted':
            sorted_sims = sorted(similarities, reverse=True)
            k = min(3, len(sorted_sims))
            top_k = sorted_sims[:k]
            return sum(top_k) / len(top_k)
        elif self.matching_strategy == 'strict':
            min_sim = min(similarities)
            avg_sim = sum(similarities) / len(similarities)
            if min_sim >= (self.similarity_threshold - 0.1):
                return avg_sim
            else:
                return min_sim
        else:
            return sum(similarities) / len(similarities)

    def detect_persons(self, frame):
        """í”„ë ˆì„ì—ì„œ ì‚¬ëŒ íƒì§€"""
        # YOLO ì „ì²˜ë¦¬
        input_data, orig_size = self.preprocess_yolo(frame)

        # YOLO ì¶”ë¡ 
        outputs = self.yolo_session.run(None, {self.yolo_input_name: input_data})

        # YOLO í›„ì²˜ë¦¬
        detections = self.postprocess_yolo(outputs, orig_size)

        return detections

    def process_video(self, video_path, output_path, progress_callback=None):
        """ì˜ìƒ ì²˜ë¦¬"""
        if not self.missing_person_embeddings:
            raise ValueError("ì‹¤ì¢…ì ì´ë¯¸ì§€ë¥¼ ë¨¼ì € ì„¤ì •í•´ì£¼ì„¸ìš”!")

        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise ValueError(f"ì˜ìƒì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")

        # ì˜ìƒ ì •ë³´
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

        # í•´ìƒë„ ì¡°ì •
        if self.resize_factor != 1.0:
            width = int(width * self.resize_factor)
            height = int(height * self.resize_factor)

        # ì¶œë ¥ ë¹„ë””ì˜¤ ì„¤ì •
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

        frame_count = 0
        processed_count = 0
        detection_count = 0
        start_time = time.time()

        print(f"\nì˜ìƒ ì²˜ë¦¬ ì‹œì‘...")
        print(f"  í•´ìƒë„: {width}x{height}")
        print(f"  ì´ í”„ë ˆì„: {total_frames}")
        print(f"  í”„ë ˆì„ ìŠ¤í‚µ: {self.frame_skip} (ì²˜ë¦¬í•  í”„ë ˆì„: {total_frames // (self.frame_skip + 1)})")
        print(f"  í•´ìƒë„ ì¶•ì†Œ: {self.resize_factor * 100:.0f}%\n")

        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break

            frame_count += 1

            # í”„ë ˆì„ ìŠ¤í‚µ
            if self.frame_skip > 0 and (frame_count - 1) % (self.frame_skip + 1) != 0:
                # í”„ë ˆì„ ì •ë³´ë§Œ í‘œì‹œí•˜ê³  ìŠ¤í‚µ
                info_text = f"Frame: {frame_count}/{total_frames} [SKIP] | Detections: {detection_count}"
                cv2.putText(frame, info_text, (10, 30),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.8, (128, 128, 128), 2)
                writer.write(frame)
                continue

            processed_count += 1

            # í•´ìƒë„ ì¡°ì •
            if self.resize_factor != 1.0:
                frame = cv2.resize(frame, (width, height))

            # ì‚¬ëŒ íƒì§€
            detections = self.detect_persons(frame)

            # íƒì§€ëœ ì‚¬ëŒë“¤ ì²˜ë¦¬
            for det in detections:
                x1, y1, x2, y2 = det['bbox']
                conf = det['confidence']

                # ì‚¬ëŒ ì˜ì—­ í¬ë¡­
                person_img = frame[y1:y2, x1:x2]
                if person_img.size == 0:
                    continue

                try:
                    # ì„ë² ë”© ì¶”ì¶œ ë° ìœ ì‚¬ë„ ê³„ì‚°
                    person_embedding = self.extract_embedding(person_img)
                    similarity = self.compute_similarity(person_embedding)

                    if similarity >= self.similarity_threshold:
                        detection_count += 1

                        # ë¹¨ê°„ìƒ‰ ë°•ìŠ¤
                        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 3)

                        label = f"MISSING PERSON! ({similarity:.2f})"
                        label_size, _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)

                        cv2.rectangle(frame,
                                    (x1, y1 - label_size[1] - 10),
                                    (x1 + label_size[0], y1),
                                    (0, 0, 255), -1)

                        cv2.putText(frame, label, (x1, y1 - 5),
                                  cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
                    else:
                        # íšŒìƒ‰ ë°•ìŠ¤
                        cv2.rectangle(frame, (x1, y1), (x2, y2), (128, 128, 128), 2)
                        cv2.putText(frame, f"{similarity:.2f}", (x1, y1 - 5),
                                  cv2.FONT_HERSHEY_SIMPLEX, 0.5, (128, 128, 128), 1)

                except Exception as e:
                    continue

            # í”„ë ˆì„ ì •ë³´ í‘œì‹œ
            info_text = f"Frame: {frame_count}/{total_frames} | Detections: {detection_count}"
            cv2.putText(frame, info_text, (10, 30),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)

            writer.write(frame)

            # ì§„í–‰ ìƒí™© ì½œë°±
            if progress_callback:
                progress = frame_count / total_frames
                elapsed = time.time() - start_time
                fps_current = processed_count / elapsed if elapsed > 0 else 0
                progress_callback(progress, frame_count, total_frames, fps_current, detection_count)

            # ì§„í–‰ ìƒí™© ì¶œë ¥
            if frame_count % 30 == 0:
                elapsed = time.time() - start_time
                fps_current = processed_count / elapsed if elapsed > 0 else 0
                print(f"ì²˜ë¦¬ ì¤‘... {frame_count}/{total_frames} í”„ë ˆì„ ({fps_current:.1f} fps)")

        cap.release()
        writer.release()

        elapsed_time = time.time() - start_time
        actual_fps = processed_count / elapsed_time if elapsed_time > 0 else 0

        print(f"\nì²˜ë¦¬ ì™„ë£Œ!")
        print(f"  ì´ í”„ë ˆì„: {frame_count}")
        print(f"  ì²˜ë¦¬ëœ í”„ë ˆì„: {processed_count}")
        print(f"  ì´ ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
        print(f"  ì‹¤ì œ ì²˜ë¦¬ FPS: {actual_fps:.2f}")
        print(f"  íƒì§€ íšŸìˆ˜: {detection_count}")

        return {
            'total_frames': frame_count,
            'processed_frames': processed_count,
            'detection_count': detection_count,
            'elapsed_time': elapsed_time,
            'avg_fps': actual_fps
        }

    def process_webcam(self, camera_index=0, max_duration=60):
        """
        ì›¹ìº  ì‹¤ì‹œê°„ ì²˜ë¦¬

        Args:
            camera_index: ì¹´ë©”ë¼ ì¸ë±ìŠ¤
            max_duration: ìµœëŒ€ ì‹¤í–‰ ì‹œê°„(ì´ˆ)

        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if not self.missing_person_embeddings:
            raise ValueError("ì‹¤ì¢…ì ì´ë¯¸ì§€ë¥¼ ë¨¼ì € ì„¤ì •í•´ì£¼ì„¸ìš”!")

        cap = cv2.VideoCapture(camera_index)
        if not cap.isOpened():
            raise ValueError(f"ì›¹ìº ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (ì¸ë±ìŠ¤: {camera_index})")

        # ì›¹ìº  ì •ë³´
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

        # í•´ìƒë„ ì¡°ì •
        if self.resize_factor != 1.0:
            width = int(width * self.resize_factor)
            height = int(height * self.resize_factor)

        frame_count = 0
        processed_count = 0
        detection_count = 0
        start_time = time.time()

        print(f"\nì›¹ìº  ì‹¤ì‹œê°„ íƒì§€ ì‹œì‘...")
        print(f"  í•´ìƒë„: {width}x{height}")
        print(f"  ìµœëŒ€ ì‹¤í–‰ ì‹œê°„: {max_duration}ì´ˆ")
        print(f"  í”„ë ˆì„ ìŠ¤í‚µ: {self.frame_skip}")
        print(f"  ì¢…ë£Œ: 'q' í‚¤\n")

        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    print("ì›¹ìº ì—ì„œ í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    break

                frame_count += 1
                elapsed = time.time() - start_time

                # ìµœëŒ€ ì‹¤í–‰ ì‹œê°„ ì²´í¬
                if max_duration and elapsed > max_duration:
                    print(f"\nìµœëŒ€ ì‹¤í–‰ ì‹œê°„ {max_duration}ì´ˆ ë„ë‹¬")
                    break

                # í”„ë ˆì„ ìŠ¤í‚µ
                if self.frame_skip > 0 and (frame_count - 1) % (self.frame_skip + 1) != 0:
                    cv2.imshow('Missing Person Detector - Webcam (ONNX)', frame)
                    if cv2.waitKey(1) & 0xFF == ord('q'):
                        break
                    continue

                processed_count += 1

                # í•´ìƒë„ ì¡°ì •
                if self.resize_factor != 1.0:
                    frame = cv2.resize(frame, (width, height))

                # ì‚¬ëŒ íƒì§€
                detections = self.detect_persons(frame)

                # íƒì§€ëœ ì‚¬ëŒë“¤ ì²˜ë¦¬
                for det in detections:
                    x1, y1, x2, y2 = det['bbox']
                    person_img = frame[y1:y2, x1:x2]
                    if person_img.size == 0:
                        continue

                    try:
                        person_embedding = self.extract_embedding(person_img)
                        similarity = self.compute_similarity(person_embedding)

                        if similarity >= self.similarity_threshold:
                            detection_count += 1

                            # ë¹¨ê°„ìƒ‰ ë°•ìŠ¤
                            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 3)

                            label = f"MISSING PERSON! ({similarity:.2f})"
                            label_size, _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)

                            cv2.rectangle(frame,
                                        (x1, y1 - label_size[1] - 10),
                                        (x1 + label_size[0], y1),
                                        (0, 0, 255), -1)

                            cv2.putText(frame, label, (x1, y1 - 5),
                                      cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
                        else:
                            # íšŒìƒ‰ ë°•ìŠ¤
                            cv2.rectangle(frame, (x1, y1), (x2, y2), (128, 128, 128), 2)
                            cv2.putText(frame, f"{similarity:.2f}", (x1, y1 - 5),
                                      cv2.FONT_HERSHEY_SIMPLEX, 0.5, (128, 128, 128), 1)

                    except Exception:
                        continue

                # ì‹¤ì‹œê°„ ì •ë³´ í‘œì‹œ
                fps_current = processed_count / elapsed if elapsed > 0 else 0
                info_text = f"FPS: {fps_current:.1f} | Time: {int(elapsed)}s | Detections: {detection_count}"
                cv2.putText(frame, info_text, (10, 30),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)

                # ìƒíƒœ í‘œì‹œ
                status = "MONITORING..." if detection_count == 0 else f"ALERT! ({detection_count} detections)"
                status_color = (0, 255, 0) if detection_count == 0 else (0, 0, 255)
                cv2.putText(frame, status, (10, height - 20),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, status_color, 2)

                # í™”ë©´ í‘œì‹œ
                cv2.imshow('Missing Person Detector - Webcam (ONNX)', frame)

                # í‚¤ ì…ë ¥
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    print("\nì‚¬ìš©ìì— ì˜í•´ ì¢…ë£Œë¨")
                    break

        finally:
            cap.release()
            cv2.destroyAllWindows()

            elapsed_time = time.time() - start_time
            actual_fps = processed_count / elapsed_time if elapsed_time > 0 else 0

            print(f"\nì›¹ìº  íƒì§€ ì¢…ë£Œ!")
            print(f"  ì´ í”„ë ˆì„: {frame_count}")
            print(f"  ì²˜ë¦¬ëœ í”„ë ˆì„: {processed_count}")
            print(f"  ì´ ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
            print(f"  ì‹¤ì œ FPS: {actual_fps:.2f}")
            print(f"  íƒì§€ íšŸìˆ˜: {detection_count}")

            return {
                'frame_count': frame_count,
                'processed_frames': processed_count,
                'detection_count': detection_count,
                'elapsed_time': elapsed_time,
                'avg_fps': actual_fps
            }

    def __del__(self):
        """ì†Œë©¸ì: ìŠ¤ë ˆë“œ í’€ ì¢…ë£Œ"""
        if hasattr(self, 'executor'):
            self.executor.shutdown(wait=False)
