"""
ìµœì í™”ëœ ì‹¤ì¢…ì íƒì§€ ì‹œìŠ¤í…œ (ONNX ê¸°ë°˜)
- 3-5ë°° ë¹ ë¥¸ ì²˜ë¦¬ ì†ë„
- í”„ë ˆì„ ìŠ¤í‚µ ì˜µì…˜
- í•´ìƒë„ ì¡°ì • ì˜µì…˜
- GPU ê°€ì† ì§€ì›
- React ìŠ¤íƒ€ì¼ ë””ìì¸ ì ìš©
"""

import streamlit as st
import cv2
import numpy as np
from PIL import Image
import tempfile
import os
import time
from missing_person_detector_onnx import MissingPersonDetectorONNX


# ì»¤ìŠ¤í…€ CSS (React ë””ìì¸ ì ìš©)
def load_custom_css():
    st.markdown("""
    <style>
    /* Import Fonts */
    @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap');

    /* Variables */
    :root {
        --primary-color: #1D70D5;
        --primary-dark: #1655a3;
        --danger-color: #ef4444;
        --background-color: #111827;
        --card-bg: rgba(17, 24, 39, 0.7);
        --text-color: #f9fafb;
        --text-secondary: #9ca3af;
        --border-color: rgba(29, 112, 213, 0.2);
    }

    /* Main Background */
    .stApp {
        background: linear-gradient(135deg, #111827, #000, #0f172a);
        font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
        color: var(--text-color);
    }

    /* Header */
    header[data-testid="stHeader"] {
        background-color: rgba(0, 0, 0, 0.2);
        backdrop-filter: blur(8px);
        border-bottom: 1px solid var(--border-color);
    }

    /* Sidebar */
    section[data-testid="stSidebar"] {
        background: linear-gradient(180deg, rgba(17, 24, 39, 0.95), rgba(0, 0, 0, 0.95));
        border-right: 1px solid var(--border-color);
    }

    section[data-testid="stSidebar"] > div {
        background-color: transparent;
    }

    /* Metrics */
    div[data-testid="stMetricValue"] {
        color: var(--primary-color);
        font-size: 2rem;
        font-weight: 700;
    }

    div[data-testid="stMetricLabel"] {
        color: var(--text-secondary);
        font-weight: 500;
    }

    /* Buttons */
    .stButton > button {
        background: linear-gradient(to right, #1D70D5, #1655a3) !important;
        color: white !important;
        border: none !important;
        border-radius: 0.75rem !important;
        padding: 0.75rem 2rem !important;
        font-weight: 600 !important;
        font-size: 1rem !important;
        box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.2) !important;
        transition: all 0.25s ease !important;
        width: 100%;
    }

    .stButton > button:hover {
        background: linear-gradient(to right, #1655a3, #12488f) !important;
        transform: translateY(-2px);
        box-shadow: 0 6px 12px -2px rgba(0, 0, 0, 0.3) !important;
    }

    /* File Uploader */
    div[data-testid="stFileUploader"] {
        background-color: var(--card-bg);
        border: 1px solid var(--border-color);
        border-radius: 1rem;
        padding: 1.5rem;
        backdrop-filter: blur(8px);
    }

    div[data-testid="stFileUploader"] label {
        color: var(--primary-color) !important;
        font-weight: 600;
    }

    /* Slider */
    div[data-testid="stSlider"] {
        background-color: rgba(29, 112, 213, 0.1);
        border: 1px solid rgba(29, 112, 213, 0.3);
        padding: 1rem;
        border-radius: 0.5rem;
    }

    div[data-testid="stSlider"] label {
        color: var(--primary-color) !important;
        font-weight: 600;
    }

    /* Select Box */
    div[data-testid="stSelectbox"] label {
        color: var(--primary-color) !important;
        font-weight: 600;
    }

    /* Radio */
    div[data-testid="stRadio"] {
        background-color: rgba(29, 112, 213, 0.1);
        border: 1px solid rgba(29, 112, 213, 0.3);
        border-radius: 0.5rem;
        padding: 1rem;
    }

    div[data-testid="stRadio"] label {
        color: var(--primary-color) !important;
        font-weight: 600;
    }

    /* Text Input */
    div[data-testid="stTextInput"] input {
        background-color: rgba(29, 112, 213, 0.1) !important;
        border: 1px solid rgba(29, 112, 213, 0.3) !important;
        border-radius: 0.5rem !important;
        color: var(--text-color) !important;
    }

    /* Checkbox */
    div[data-testid="stCheckbox"] label {
        color: var(--text-color) !important;
        font-weight: 500;
    }

    /* Alerts */
    div[data-testid="stAlert"] {
        background-color: var(--card-bg);
        border-radius: 0.75rem;
        border: 1px solid var(--border-color);
        backdrop-filter: blur(8px);
    }

    /* Success */
    div.stSuccess {
        background-color: rgba(34, 197, 94, 0.1) !important;
        border: 1px solid rgba(34, 197, 94, 0.3) !important;
    }

    /* Warning */
    div.stWarning {
        background-color: rgba(251, 191, 36, 0.1) !important;
        border: 1px solid rgba(251, 191, 36, 0.3) !important;
    }

    /* Error */
    div.stError {
        background-color: rgba(239, 68, 68, 0.1) !important;
        border: 1px solid rgba(239, 68, 68, 0.3) !important;
    }

    /* Info */
    div.stInfo {
        background-color: rgba(29, 112, 213, 0.1) !important;
        border: 1px solid rgba(29, 112, 213, 0.3) !important;
    }

    /* Progress */
    div[data-testid="stProgress"] > div {
        background-color: rgba(29, 112, 213, 0.2);
        border-radius: 0.5rem;
    }

    div[data-testid="stProgress"] > div > div {
        background: linear-gradient(to right, #1D70D5, #1655a3);
    }

    /* Image/Video */
    div[data-testid="stImage"], div[data-testid="stVideo"] {
        border-radius: 1rem;
        overflow: hidden;
        box-shadow: 0 10px 15px -3px rgba(0, 0, 0, 0.3);
    }

    /* Expander */
    div[data-testid="stExpander"] {
        background-color: var(--card-bg);
        border: 1px solid var(--border-color);
        border-radius: 1rem;
        backdrop-filter: blur(8px);
    }

    /* Headers */
    h1 {
        background: linear-gradient(to right, #1D70D5, #60a5fa);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        background-clip: text;
        font-weight: 700;
    }

    h2, h3 {
        color: var(--primary-color);
        font-weight: 600;
    }

    /* Code */
    code {
        background-color: rgba(29, 112, 213, 0.1);
        color: var(--primary-color);
        padding: 0.2rem 0.4rem;
        border-radius: 0.25rem;
    }

    /* Scrollbar */
    ::-webkit-scrollbar {
        width: 8px;
        height: 8px;
    }

    ::-webkit-scrollbar-track {
        background: rgba(17, 24, 39, 0.5);
    }

    ::-webkit-scrollbar-thumb {
        background: var(--primary-color);
        border-radius: 4px;
    }

    ::-webkit-scrollbar-thumb:hover {
        background: var(--primary-dark);
    }

    /* Hide Streamlit branding */
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    </style>
    """, unsafe_allow_html=True)


def main():
    st.set_page_config(
        page_title="ì‹¤ì¢…ì íƒì§€ ì‹œìŠ¤í…œ (ONNX ìµœì í™”)",
        page_icon="âš¡",
        layout="wide"
    )

    # ì»¤ìŠ¤í…€ CSS ë¡œë“œ
    load_custom_css()

    st.title("âš¡ ì‹¤ì¢…ì ì‹¤ì‹œê°„ íƒì§€ ì‹œìŠ¤í…œ (ONNX ìµœì í™” - ë¸Œë¼ìš°ì € ìŠ¤íŠ¸ë¦¬ë°)")
    st.caption("ğŸš€ PyTorch ëŒ€ë¹„ 3-5ë°° ë¹ ë¥¸ ì²˜ë¦¬ ì†ë„ | ONNX Runtime | ğŸ“± ë¸Œë¼ìš°ì €ì—ì„œ ë°”ë¡œ ë³´ê¸°")
    st.markdown("---")

    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.header("âš™ï¸ ì„¤ì •")

        # ONNX íŒŒì¼ í™•ì¸
        yolo_exists = os.path.exists('yolov8n.onnx')
        osnet_exists = os.path.exists('osnet_x1_0.onnx')

        if not yolo_exists or not osnet_exists:
            st.error("âŒ ONNX ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤!")
            st.warning("ë¨¼ì € ë‹¤ìŒ ëª…ë ¹ì„ ì‹¤í–‰í•˜ì„¸ìš”:")
            st.code("python convert_to_onnx.py", language="bash")
            st.stop()
        else:
            st.success("âœ… ONNX ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ")

        st.markdown("---")

        # ì‹¤ì¢…ì ì´ë¯¸ì§€ ì—…ë¡œë“œ
        st.subheader("1ï¸âƒ£ ì‹¤ì¢…ì ì´ë¯¸ì§€")
        uploaded_images = st.file_uploader(
            "ì‹¤ì¢…ì ì‚¬ì§„ì„ ì—…ë¡œë“œí•˜ì„¸ìš” (ì—¬ëŸ¬ ì¥ ê¶Œì¥)",
            type=['jpg', 'jpeg', 'png'],
            accept_multiple_files=True,
            help="ë‹¤ì–‘í•œ ê°ë„/ì¡°ëª…ì˜ ì‚¬ì§„ì„ ì—¬ëŸ¬ ì¥ ì—…ë¡œë“œí•˜ë©´ ì •í™•ë„ê°€ í–¥ìƒë©ë‹ˆë‹¤"
        )

        if uploaded_images:
            st.write(f"ğŸ“¸ ì—…ë¡œë“œëœ ì´ë¯¸ì§€: {len(uploaded_images)}ì¥")
            cols = st.columns(min(len(uploaded_images), 3))
            for idx, uploaded_image in enumerate(uploaded_images):
                with cols[idx % 3]:
                    image = Image.open(uploaded_image).convert('RGB')
                    st.image(image, caption=f"ì´ë¯¸ì§€ {idx+1}", use_container_width=True)

        st.markdown("---")

        # ì…ë ¥ ì†ŒìŠ¤ ì„ íƒ
        st.subheader("2ï¸âƒ£ ì…ë ¥ ì†ŒìŠ¤")
        input_source = st.radio(
            "ì…ë ¥ ì†ŒìŠ¤ë¥¼ ì„ íƒí•˜ì„¸ìš”",
            ["ğŸ“ ë¹„ë””ì˜¤ íŒŒì¼", "ğŸ“· ì›¹ìº  (ì‹¤ì‹œê°„)"],
            help="ë¹„ë””ì˜¤ íŒŒì¼ ë˜ëŠ” ì›¹ìº  ì¤‘ ì„ íƒí•˜ì„¸ìš”"
        )

        uploaded_video = None
        camera_index = 0
        max_duration = None

        if input_source == "ğŸ“ ë¹„ë””ì˜¤ íŒŒì¼":
            # ì˜ìƒ íŒŒì¼ ì—…ë¡œë“œ
            uploaded_video = st.file_uploader(
                "CCTV ì˜ìƒì„ ì—…ë¡œë“œí•˜ì„¸ìš”",
                type=['mp4', 'avi', 'mov'],
                help="ë¶„ì„í•  CCTV ì˜ìƒ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”"
            )
        else:
            # ì›¹ìº  ì„¤ì •
            camera_type = st.radio(
                "ì¹´ë©”ë¼ íƒ€ì…",
                ["ğŸ’» PC ì›¹ìº ", "ğŸ“± íœ´ëŒ€í° (IP Webcam)"],
                help="PC ë‚´ì¥ ì¹´ë©”ë¼ ë˜ëŠ” íœ´ëŒ€í° IP ì¹´ë©”ë¼ ì„ íƒ"
            )

            if camera_type == "ğŸ’» PC ì›¹ìº ":
                camera_index = st.selectbox(
                    "ì¹´ë©”ë¼ ì„ íƒ",
                    options=[0, 1, 2],
                    help="ì‚¬ìš©í•  ì¹´ë©”ë¼ ì¸ë±ìŠ¤ (0 = ê¸°ë³¸ ì¹´ë©”ë¼)"
                )
            else:
                # íœ´ëŒ€í° IP ì¹´ë©”ë¼
                st.info("""
                **ğŸ“± IP Webcam ì•± ì‚¬ìš©ë²•:**
                1. íœ´ëŒ€í°ì—ì„œ IP Webcam ì•± ì‹¤í–‰
                2. í•˜ë‹¨ "ì„œë²„ ì‹œì‘" í´ë¦­
                3. í™”ë©´ ìƒë‹¨ì— í‘œì‹œëœ ì£¼ì†Œë¥¼ ì•„ë˜ì— ì…ë ¥
                """)

                ip_input = st.text_input(
                    "IP Webcam ì£¼ì†Œ",
                    value="192.168.0.106:8080",
                    help="ì˜ˆ: 192.168.0.106:8080"
                )

                # http:// ì™€ /video ìë™ ì¶”ê°€
                if ip_input:
                    if not ip_input.startswith('http'):
                        ip_input = 'http://' + ip_input
                    if not ip_input.endswith('/video'):
                        ip_input = ip_input + '/video'

                    camera_index = ip_input
                    st.success(f"âœ… ì—°ê²° ì£¼ì†Œ: `{camera_index}`")
                else:
                    camera_index = "http://192.168.0.106:8080/video"

            st.caption("ğŸ’¡ ì›¹ìº ì€ 'q' í‚¤ë¥¼ ëˆŒëŸ¬ ì¢…ë£Œí•  ë•Œê¹Œì§€ ê³„ì† ì‹¤í–‰ë©ë‹ˆë‹¤")
            max_duration = None  # ë¬´ì œí•œ

        st.markdown("---")

        # íƒì§€ ì„¤ì •
        st.subheader("3ï¸âƒ£ íƒì§€ ì„¤ì •")

        # GPU ì‚¬ìš© ì—¬ë¶€
        use_gpu = st.checkbox(
            "GPU ì‚¬ìš©",
            value=True,
            help="GPUë¥¼ ì‚¬ìš©í•˜ë©´ ë” ë¹ ë¦…ë‹ˆë‹¤ (CUDA í•„ìš”)"
        )

        # ë§¤ì¹­ ì „ëµ
        matching_strategy = st.selectbox(
            "ë§¤ì¹­ ì „ëµ",
            options=['average', 'weighted', 'strict', 'max'],
            index=0,
            help="ì—¬ëŸ¬ ì°¸ì¡° ì´ë¯¸ì§€ë¥¼ ì–´ë–»ê²Œ ë¹„êµí• ì§€ ì„ íƒ"
        )

        strategy_info = {
            'average': 'í‰ê· : ëª¨ë“  ì´ë¯¸ì§€ì˜ í‰ê·  ìœ ì‚¬ë„ (ê¶Œì¥)',
            'weighted': 'ê°€ì¤‘: ìƒìœ„ 3ê°œì˜ í‰ê·  (ê· í˜•)',
            'strict': 'ì—„ê²©: ëª¨ë“  ì´ë¯¸ì§€ê°€ ë†’ì•„ì•¼ í•¨',
            'max': 'ìµœëŒ€: í•˜ë‚˜ë¼ë„ ë†’ìœ¼ë©´ íƒì§€ (ì˜¤íƒ ìœ„í—˜)'
        }
        st.caption(f"âœ“ {strategy_info[matching_strategy]}")

        # ìœ ì‚¬ë„ ì„ê³„ê°’
        similarity_threshold = st.slider(
            "ìœ ì‚¬ë„ ì„ê³„ê°’",
            min_value=0.5,
            max_value=0.95,
            value=0.75,
            step=0.05,
            help="ë†’ì„ìˆ˜ë¡ ì—„ê²©í•˜ê²Œ íƒì§€í•©ë‹ˆë‹¤"
        )

        st.markdown("---")

        # ì„±ëŠ¥ ìµœì í™” ì„¤ì •
        st.subheader("4ï¸âƒ£ ì„±ëŠ¥ ìµœì í™”")

        # í”„ë ˆì„ ìŠ¤í‚µ
        frame_skip = st.slider(
            "í”„ë ˆì„ ìŠ¤í‚µ (ì†ë„ í–¥ìƒ)",
            min_value=0,
            max_value=5,
            value=0,
            step=1,
            help="0=ëª¨ë“  í”„ë ˆì„, 1=1í”„ë ˆì„ ê±´ë„ˆë›°ê¸°, 2=2í”„ë ˆì„ ê±´ë„ˆë›°ê¸°"
        )

        if frame_skip > 0:
            st.caption(f"âš¡ {frame_skip}í”„ë ˆì„ë§ˆë‹¤ ê±´ë„ˆë›°ê¸° â†’ ì•½ {(frame_skip + 1)}ë°° ë¹ ë¦„")
        else:
            st.caption("âœ“ ëª¨ë“  í”„ë ˆì„ ì²˜ë¦¬ (ê°€ì¥ ì •í™•)")

        # í•´ìƒë„ ì¡°ì •
        resize_factor = st.slider(
            "í•´ìƒë„ ì¡°ì • (ë©”ëª¨ë¦¬ ìµœì í™”)",
            min_value=0.25,
            max_value=1.0,
            value=1.0,
            step=0.05,
            help="1.0=ì›ë³¸, 0.5=50% ì¶•ì†Œ, 0.25=25% ì¶•ì†Œ"
        )

        if resize_factor < 1.0:
            st.caption(f"âš¡ í•´ìƒë„ {resize_factor*100:.0f}% â†’ ì•½ {(1/resize_factor)**2:.1f}ë°° ë¹ ë¦„")
        else:
            st.caption("âœ“ ì›ë³¸ í•´ìƒë„ ìœ ì§€ (ê°€ì¥ ì •í™•)")

        # ì˜ˆìƒ ì†ë„ í–¥ìƒ
        total_speedup = 3.0  # ONNX ê¸°ë³¸ ì†ë„ í–¥ìƒ
        if frame_skip > 0:
            total_speedup *= (frame_skip + 1)
        if resize_factor < 1.0:
            total_speedup *= (1 / resize_factor) ** 1.5

        st.info(f"ğŸš€ **ì˜ˆìƒ ì†ë„ í–¥ìƒ**: PyTorch ëŒ€ë¹„ ì•½ **{total_speedup:.1f}ë°°** ë¹ ë¦„")

    # ë©”ì¸ ì˜ì—­
    col1, col2 = st.columns([1, 1])

    with col1:
        st.subheader("ğŸ“‹ ì‹œìŠ¤í…œ ìƒíƒœ")

        if input_source == "ğŸ“ ë¹„ë””ì˜¤ íŒŒì¼":
            if uploaded_images and uploaded_video:
                st.success(f"âœ… ëª¨ë“  íŒŒì¼ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤! (ì‹¤ì¢…ì ì´ë¯¸ì§€: {len(uploaded_images)}ì¥)")

                # ì˜ìƒ ì •ë³´ í‘œì‹œ
                with tempfile.NamedTemporaryFile(delete=False, suffix='.mp4') as tmp_video:
                    tmp_video.write(uploaded_video.read())
                    tmp_video_path = tmp_video.name

                cap = cv2.VideoCapture(tmp_video_path)
                width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
                height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
                fps = int(cap.get(cv2.CAP_PROP_FPS))
                total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
                cap.release()

                # ìµœì í™” í›„ ì˜ˆìƒ í•´ìƒë„
                opt_width = int(width * resize_factor)
                opt_height = int(height * resize_factor)
                opt_frames = total_frames // (frame_skip + 1) if frame_skip > 0 else total_frames

                st.info(f"""
                **ì›ë³¸ ì˜ìƒ ì •ë³´:**
                - í•´ìƒë„: {width}x{height}
                - FPS: {fps}
                - ì´ í”„ë ˆì„: {total_frames}

                **ìµœì í™” í›„:**
                - ì²˜ë¦¬ í•´ìƒë„: {opt_width}x{opt_height}
                - ì²˜ë¦¬ í”„ë ˆì„: {opt_frames}
                - ì˜ˆìƒ ì†Œìš” ì‹œê°„: {opt_frames / (total_speedup * 10):.0f}ì´ˆ
                """)

            elif not uploaded_images:
                st.warning("âš ï¸ ì‹¤ì¢…ì ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”")
            elif not uploaded_video:
                st.warning("âš ï¸ CCTV ì˜ìƒì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”")
        else:
            # ì›¹ìº  ëª¨ë“œ
            if uploaded_images:
                st.success(f"âœ… ì‹¤ì¢…ì ì´ë¯¸ì§€ ì¤€ë¹„ ì™„ë£Œ! ({len(uploaded_images)}ì¥)")

                camera_display = camera_index if isinstance(camera_index, str) else f"ì¹´ë©”ë¼ {camera_index}"
                st.info(f"""
                **ì›¹ìº  ì„¤ì •:**
                - ì¹´ë©”ë¼: {camera_display}
                - ì‹¤í–‰ ì‹œê°„: ë¬´ì œí•œ ('q' í‚¤ë¡œ ì¢…ë£Œ)
                - ìœ ì‚¬ë„ ì„ê³„ê°’: {similarity_threshold}
                - í”„ë ˆì„ ìŠ¤í‚µ: {frame_skip}
                - í•´ìƒë„ ì¡°ì •: {resize_factor * 100:.0f}%
                """)
            else:
                st.warning("âš ï¸ ì‹¤ì¢…ì ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”")

    with col2:
        st.subheader("ğŸš€ íƒì§€ ì‹œì‘")

        if input_source == "ğŸ“ ë¹„ë””ì˜¤ íŒŒì¼":
            # ë¹„ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬
            if st.button("ğŸ” ì‹¤ì¢…ì íƒì§€ ì‹œì‘ (ONNX ê°€ì†)", type="primary", use_container_width=True):
                if not uploaded_images or not uploaded_video:
                    st.error("âŒ ì´ë¯¸ì§€ì™€ ì˜ìƒì„ ëª¨ë‘ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”!")
                else:
                    try:
                        # ì„ì‹œ íŒŒì¼ ìƒì„±
                        with tempfile.NamedTemporaryFile(delete=False, suffix='.mp4') as tmp_video:
                            uploaded_video.seek(0)
                            tmp_video.write(uploaded_video.read())
                            tmp_video_path = tmp_video.name

                        output_path = tempfile.mktemp(suffix='.mp4')

                        # ONNX íƒì§€ê¸° ì´ˆê¸°í™”
                        with st.spinner("ONNX ëª¨ë¸ ë¡œë”© ì¤‘..."):
                            detector = MissingPersonDetectorONNX(
                                yolo_onnx_path='yolov8n.onnx',
                                osnet_onnx_path='osnet_x1_0.onnx',
                                similarity_threshold=similarity_threshold,
                                matching_strategy=matching_strategy,
                                frame_skip=frame_skip,
                                resize_factor=resize_factor,
                                use_gpu=use_gpu
                            )

                        # ì‹¤ì¢…ì ì´ë¯¸ì§€ ì„¤ì •
                        images = []
                        for uploaded_img in uploaded_images:
                            uploaded_img.seek(0)
                            image = Image.open(uploaded_img).convert('RGB')
                            images.append(image)

                        if len(images) == 1:
                            detector.set_missing_person(images[0])
                        else:
                            detector.set_missing_persons(images)

                        # ì§„í–‰ ìƒí™© í‘œì‹œ
                        progress_bar = st.progress(0)
                        status_text = st.empty()

                        def progress_callback(progress, frame_count, total_frames, fps_current, detection_count):
                            progress_bar.progress(progress)
                            status_text.text(
                                f"âš¡ ì²˜ë¦¬ ì¤‘... {frame_count}/{total_frames} í”„ë ˆì„ | "
                                f"{fps_current:.1f} fps | íƒì§€: {detection_count}íšŒ"
                            )

                        # ì˜ìƒ ì²˜ë¦¬
                        start_time = time.time()
                        with st.spinner("ğŸš€ ì˜ìƒ ì²˜ë¦¬ ì¤‘... (ONNX ê°€ì†)"):
                            results = detector.process_video(
                                tmp_video_path,
                                output_path,
                                progress_callback=progress_callback
                            )

                        processing_time = time.time() - start_time

                        # ê²°ê³¼ í‘œì‹œ
                        st.success("âœ… íƒì§€ ì™„ë£Œ!")

                        col_r1, col_r2, col_r3, col_r4 = st.columns(4)
                        with col_r1:
                            st.metric("ì´ í”„ë ˆì„", f"{results['total_frames']:,}")
                        with col_r2:
                            st.metric("ì²˜ë¦¬ í”„ë ˆì„", f"{results['processed_frames']:,}")
                        with col_r3:
                            st.metric("íƒì§€ íšŸìˆ˜", f"{results['detection_count']:,}")
                        with col_r4:
                            st.metric("í‰ê·  FPS", f"{results['avg_fps']:.1f}")

                        st.info(f"â±ï¸ ì²˜ë¦¬ ì‹œê°„: {processing_time:.1f}ì´ˆ")

                        # ì„±ëŠ¥ ë¹„êµ
                        estimated_pytorch_time = processing_time * total_speedup
                        st.success(
                            f"ğŸš€ **ONNX ê°€ì† íš¨ê³¼**: PyTorchë¡œ í–ˆë‹¤ë©´ ì•½ {estimated_pytorch_time:.1f}ì´ˆ "
                            f"ê±¸ë ¸ì„ ê²ƒì„ {processing_time:.1f}ì´ˆë§Œì— ì™„ë£Œ! "
                            f"(**{total_speedup:.1f}ë°°** ë¹ ë¦„)"
                        )

                        # ê²°ê³¼ ì˜ìƒ í‘œì‹œ
                        st.subheader("ğŸ“¹ ê²°ê³¼ ì˜ìƒ")

                        if os.path.exists(output_path):
                            with open(output_path, 'rb') as f:
                                video_bytes = f.read()

                            st.video(video_bytes)

                            # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
                            st.download_button(
                                label="â¬‡ï¸ ê²°ê³¼ ì˜ìƒ ë‹¤ìš´ë¡œë“œ",
                                data=video_bytes,
                                file_name=f"detected_onnx_{int(time.time())}.mp4",
                                mime="video/mp4",
                                use_container_width=True
                            )

                            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                            os.unlink(output_path)

                        os.unlink(tmp_video_path)

                    except Exception as e:
                        st.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                        import traceback
                        st.code(traceback.format_exc())

        else:
            # ì›¹ìº  ì‹¤ì‹œê°„ íƒì§€ (ë¸Œë¼ìš°ì € ìŠ¤íŠ¸ë¦¬ë°)
            col_btn1, col_btn2 = st.columns([1, 1])

            with col_btn1:
                start_btn = st.button("ğŸ“· ì›¹ìº  íƒì§€ ì‹œì‘", type="primary", use_container_width=True)

            with col_btn2:
                if 'webcam_running' in st.session_state and st.session_state.webcam_running:
                    if st.button("â¹ï¸ ì¤‘ì§€", type="secondary", use_container_width=True):
                        st.session_state.webcam_running = False
                        st.rerun()

            if start_btn:
                if not uploaded_images:
                    st.error("âŒ ì‹¤ì¢…ì ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”!")
                else:
                    st.success("âœ… ì›¹ìº  íƒì§€ ì‹œì‘! (ë¸Œë¼ìš°ì €ì—ì„œ ì‹¤ì‹œê°„ í‘œì‹œ)")
                    st.info("ğŸ’¡ 'â¹ï¸ ì¤‘ì§€' ë²„íŠ¼ì„ ëˆŒëŸ¬ ì¢…ë£Œí•˜ì„¸ìš”")

                    try:
                        # ONNX íƒì§€ê¸° ì´ˆê¸°í™”
                        with st.spinner("ONNX ëª¨ë¸ ë¡œë”© ì¤‘..."):
                            detector = MissingPersonDetectorONNX(
                                yolo_onnx_path='yolov8n.onnx',
                                osnet_onnx_path='osnet_x1_0.onnx',
                                similarity_threshold=similarity_threshold,
                                matching_strategy=matching_strategy,
                                frame_skip=frame_skip,
                                resize_factor=resize_factor,
                                use_gpu=use_gpu
                            )

                        # ì‹¤ì¢…ì ì´ë¯¸ì§€ ì„¤ì •
                        images = []
                        for uploaded_img in uploaded_images:
                            uploaded_img.seek(0)
                            image = Image.open(uploaded_img).convert('RGB')
                            images.append(image)

                        if len(images) == 1:
                            detector.set_missing_person(images[0])
                        else:
                            detector.set_missing_persons(images)

                        # ì›¹ìº  ì—´ê¸°
                        cap = cv2.VideoCapture(camera_index)

                        if not cap.isOpened():
                            st.error(f"âŒ ì¹´ë©”ë¼ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {camera_index}")
                        else:
                            # í•´ìƒë„
                            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
                            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

                            if resize_factor != 1.0:
                                width = int(width * resize_factor)
                                height = int(height * resize_factor)

                            # ìŠ¤íŠ¸ë¦¬ë° ì˜ì—­
                            frame_placeholder = st.empty()
                            status_placeholder = st.empty()
                            metrics_placeholder = st.empty()

                            frame_count = 0
                            processed_count = 0
                            detection_count = 0
                            start_time = time.time()

                            st.session_state.webcam_running = True

                            # ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ë£¨í”„
                            while st.session_state.get('webcam_running', False):
                                ret, frame = cap.read()
                                if not ret:
                                    st.error("ì›¹ìº ì—ì„œ í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                                    break

                                frame_count += 1
                                elapsed = time.time() - start_time

                                # í”„ë ˆì„ ìŠ¤í‚µ
                                if frame_skip > 0 and (frame_count - 1) % (frame_skip + 1) != 0:
                                    continue

                                processed_count += 1

                                # í•´ìƒë„ ì¡°ì •
                                if resize_factor != 1.0:
                                    frame = cv2.resize(frame, (width, height))

                                # ì‚¬ëŒ íƒì§€
                                detections = detector.detect_persons(frame)

                                # íƒì§€ëœ ì‚¬ëŒë“¤ ì²˜ë¦¬
                                for det in detections:
                                    x1, y1, x2, y2 = det['bbox']
                                    person_img = frame[y1:y2, x1:x2]
                                    if person_img.size == 0:
                                        continue

                                    try:
                                        person_embedding = detector.extract_embedding(person_img)
                                        similarity = detector.compute_similarity(person_embedding)

                                        if similarity >= similarity_threshold:
                                            detection_count += 1

                                            # ë¹¨ê°„ìƒ‰ ë°•ìŠ¤
                                            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 3)

                                            label = f"MISSING PERSON! ({similarity:.2f})"
                                            label_size, _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)

                                            cv2.rectangle(frame,
                                                        (x1, y1 - label_size[1] - 10),
                                                        (x1 + label_size[0], y1),
                                                        (0, 0, 255), -1)

                                            cv2.putText(frame, label, (x1, y1 - 5),
                                                      cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
                                        else:
                                            # íšŒìƒ‰ ë°•ìŠ¤
                                            cv2.rectangle(frame, (x1, y1), (x2, y2), (128, 128, 128), 2)
                                            cv2.putText(frame, f"{similarity:.2f}", (x1, y1 - 5),
                                                      cv2.FONT_HERSHEY_SIMPLEX, 0.5, (128, 128, 128), 1)

                                    except Exception:
                                        continue

                                # ì‹¤ì‹œê°„ ì •ë³´ í‘œì‹œ
                                fps_current = processed_count / elapsed if elapsed > 0 else 0
                                info_text = f"FPS: {fps_current:.1f} | Time: {int(elapsed)}s | Detections: {detection_count}"
                                cv2.putText(frame, info_text, (10, 30),
                                           cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)

                                # ìƒíƒœ í‘œì‹œ
                                status = "MONITORING..." if detection_count == 0 else f"ALERT! ({detection_count} detections)"
                                status_color = (0, 255, 0) if detection_count == 0 else (0, 0, 255)
                                cv2.putText(frame, status, (10, height - 20),
                                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, status_color, 2)

                                # ë¸Œë¼ìš°ì €ì— í”„ë ˆì„ í‘œì‹œ (BGR -> RGB)
                                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                                frame_placeholder.image(frame_rgb, channels="RGB", use_container_width=True)

                                # ìƒíƒœ ì—…ë°ì´íŠ¸
                                status_placeholder.text(f"âš¡ ì‹¤ì‹œê°„ íƒì§€ ì¤‘... {fps_current:.1f} fps | íƒì§€: {detection_count}íšŒ")

                                # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
                                with metrics_placeholder.container():
                                    col_m1, col_m2, col_m3 = st.columns(3)
                                    with col_m1:
                                        st.metric("ì²˜ë¦¬ í”„ë ˆì„", f"{processed_count:,}")
                                    with col_m2:
                                        st.metric("íƒì§€ íšŸìˆ˜", f"{detection_count:,}")
                                    with col_m3:
                                        st.metric("FPS", f"{fps_current:.1f}")

                            # ì¢…ë£Œ
                            cap.release()
                            st.session_state.webcam_running = False

                            # ìµœì¢… ê²°ê³¼
                            elapsed_time = time.time() - start_time
                            st.success("âœ… ì›¹ìº  íƒì§€ ì™„ë£Œ!")
                            st.info(f"â±ï¸ ì´ ì‹¤í–‰ ì‹œê°„: {elapsed_time:.1f}ì´ˆ | í‰ê·  FPS: {processed_count/elapsed_time:.1f}")

                            if detection_count > 0:
                                st.warning(f"âš ï¸ **ê²½ê³ **: ì‹¤ì¢…ìê°€ {detection_count}íšŒ íƒì§€ë˜ì—ˆìŠµë‹ˆë‹¤!")

                    except Exception as e:
                        st.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                        import traceback
                        st.code(traceback.format_exc())
                        st.session_state.webcam_running = False

    # í•˜ë‹¨ ì •ë³´
    st.markdown("---")
    with st.expander("â„¹ï¸ ì‚¬ìš© ë°©ë²• ë° ìµœì í™” ê°€ì´ë“œ"):
        st.markdown("""
        ## ğŸ“ ê¸°ë³¸ ì‚¬ìš©ë²•
        1. **ì™¼ìª½ ì‚¬ì´ë“œë°”**ì—ì„œ ì‹¤ì¢…ì ì´ë¯¸ì§€ ì—…ë¡œë“œ (ì—¬ëŸ¬ ì¥ ê¶Œì¥)
        2. CCTV ì˜ìƒ íŒŒì¼ ì—…ë¡œë“œ (mp4, avi, mov)
        3. íƒì§€ ì„¤ì • ë° ì„±ëŠ¥ ìµœì í™” ì˜µì…˜ ì¡°ì •
        4. **íƒì§€ ì‹œì‘** ë²„íŠ¼ í´ë¦­
        5. ê²°ê³¼ ì˜ìƒ í™•ì¸ ë° ë‹¤ìš´ë¡œë“œ

        ## âš¡ ì„±ëŠ¥ ìµœì í™” ê°€ì´ë“œ

        ### í”„ë ˆì„ ìŠ¤í‚µ
        - **0**: ëª¨ë“  í”„ë ˆì„ ì²˜ë¦¬ (ê°€ì¥ ì •í™•, ëŠë¦¼)
        - **1**: 1í”„ë ˆì„ ê±´ë„ˆë›°ê¸° (2ë°° ë¹ ë¦„, ì •í™•ë„ 90%)
        - **2**: 2í”„ë ˆì„ ê±´ë„ˆë›°ê¸° (3ë°° ë¹ ë¦„, ì •í™•ë„ 80%)
        - **ê¶Œì¥**: ë¹ ë¥¸ ì›€ì§ì„ì´ ì—†ëŠ” CCTVëŠ” 1-2 ì¶”ì²œ

        ### í•´ìƒë„ ì¡°ì •
        - **1.0**: ì›ë³¸ í•´ìƒë„ ìœ ì§€ (ê°€ì¥ ì •í™•)
        - **0.75**: 75% ì¶•ì†Œ (1.7ë°° ë¹ ë¦„, ì •í™•ë„ 95%)
        - **0.5**: 50% ì¶•ì†Œ (4ë°° ë¹ ë¦„, ì •í™•ë„ 85%)
        - **ê¶Œì¥**: ê³ í•´ìƒë„ ì˜ìƒ(1080p ì´ìƒ)ì€ 0.75 ì¶”ì²œ

        ### GPU ì‚¬ìš©
        - **í™œì„±í™”**: CUDA GPU ì‚¬ìš© (3-5ë°° ë¹ ë¦„)
        - **ë¹„í™œì„±í™”**: CPUë§Œ ì‚¬ìš© (ëŠë¦¬ì§€ë§Œ ì•ˆì •ì )
        - **ê¶Œì¥**: GPUê°€ ìˆìœ¼ë©´ ë°˜ë“œì‹œ í™œì„±í™”

        ## ğŸ¯ ìµœì  ì„¤ì • ì˜ˆì‹œ

        ### ë¹ ë¥¸ ì²˜ë¦¬ (5-10ë°° ì†ë„ í–¥ìƒ)
        - í”„ë ˆì„ ìŠ¤í‚µ: 2
        - í•´ìƒë„: 0.5
        - GPU: í™œì„±í™”
        - ìš©ë„: ëŒ€ìš©ëŸ‰ ì˜ìƒ ë¹ ë¥¸ ìŠ¤ìº”

        ### ê· í˜•ì¡íŒ ì„¤ì • (3-5ë°° ì†ë„ í–¥ìƒ)
        - í”„ë ˆì„ ìŠ¤í‚µ: 1
        - í•´ìƒë„: 0.75
        - GPU: í™œì„±í™”
        - ìš©ë„: ì¼ë°˜ì ì¸ CCTV ë¶„ì„ (ê¶Œì¥)

        ### ì •í™•í•œ ì²˜ë¦¬ (3ë°° ì†ë„ í–¥ìƒ)
        - í”„ë ˆì„ ìŠ¤í‚µ: 0
        - í•´ìƒë„: 1.0
        - GPU: í™œì„±í™”
        - ìš©ë„: ì¤‘ìš”í•œ ì˜ìƒ, ê³ ì •ë°€ íƒì§€ í•„ìš”

        ## ğŸ’¡ íŒ
        - ONNXë§Œìœ¼ë¡œë„ PyTorch ëŒ€ë¹„ 3ë°° ë¹ ë¦„
        - í”„ë ˆì„ ìŠ¤í‚µ + í•´ìƒë„ ì¡°ì •ìœ¼ë¡œ ìµœëŒ€ 10ë°° ì´ìƒ ê°€ì† ê°€ëŠ¥
        - ì‹¤ì‹œê°„ CCTVëŠ” í”„ë ˆì„ ìŠ¤í‚µ 1-2 ê¶Œì¥ (ì‚¬ëŒì´ í¬ê²Œ ì›€ì§ì´ì§€ ì•ŠìŒ)
        - GPUê°€ ì—†ì–´ë„ ONNX CPUê°€ PyTorch CPUë³´ë‹¤ 2-3ë°° ë¹ ë¦„
        """)

    with st.expander("âš™ï¸ ê¸°ìˆ  ì •ë³´"):
        st.markdown("""
        ### ONNX ìµœì í™” ê¸°ìˆ 
        - **ONNX Runtime**: ë”¥ëŸ¬ë‹ ëª¨ë¸ ì¶”ë¡  ìµœì í™” ì—”ì§„
        - **Graph Optimization**: ê³„ì‚° ê·¸ë˜í”„ ìµœì í™”
        - **Quantization Ready**: INT8 ì–‘ìí™” ì§€ì› (ì¶”ê°€ 2ë°° ì†ë„ í–¥ìƒ ê°€ëŠ¥)
        - **Multi-threading**: CPU ë³‘ë ¬ ì²˜ë¦¬

        ### ì„±ëŠ¥ í–¥ìƒ ìš”ì•½
        - YOLOv8: PyTorch ëŒ€ë¹„ 2-3ë°° ë¹ ë¦„
        - OSNet: PyTorch ëŒ€ë¹„ 1.5-2ë°° ë¹ ë¦„
        - ì „ì²´: ì•½ 3ë°° ê¸°ë³¸ ì†ë„ í–¥ìƒ (GPU ê¸°ì¤€)

        ### ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­
        - CPU: ëª¨ë“  í”„ë¡œì„¸ì„œ ì§€ì› (AVX2 ê¶Œì¥)
        - GPU: NVIDIA GPU + CUDA (ì„ íƒì‚¬í•­, í° ì†ë„ í–¥ìƒ)
        - RAM: 4GB ì´ìƒ (8GB ê¶Œì¥)
        - VRAM: 2GB ì´ìƒ (GPU ì‚¬ìš© ì‹œ)
        """)


if __name__ == "__main__":
    main()
